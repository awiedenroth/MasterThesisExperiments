import pandas as pd
import numpy as np
from pathlib import Path
from copy import deepcopy
import fastText as fasttext
import fasttext.util
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from nltk.tokenize.treebank import TreebankWordDetokenizer
import nlpaug.augmenter.char as nac
import nlpaug.augmenter.word as naw
import nlpaug.augmenter.sentence as nas


nltk.download('stopwords')



ft = fasttext.load_model('cc.de.300.bin')


def missing_val(x):
    if x in [".a", ".b", ".c", ".d"]:
        x = -1
    return int(x)

# wie Ã¼bergebe ich die Daten? Ich brauche ja X und y, wie gehe ich damit um? Ich muss sicherstellen, dass x und y immernoch
# zusammen gehÃ¶ren ganz am Ende, wie kann ich das tun???
class Augmentierer:

    def __init__(self, fasttext_df, configuration):

        self.fasttext_df = fasttext_df
        self.config = configuration


    def augment_data(self):
        # fasttext_df ist ein df mit 3 Spalten: "taetigk","embeddings", self.oesch
        # hier nehmen wir nun die spalte taetigk, modifizieren sie, leiten daraus embeddings ab und fÃ¼gen dann mit dem
        # identischen label den neuen Datenpunkt hinzu
        # Erinnerung: wir iterieren nicht durch df, sondern wenden funktion an auf alle zeilen

        #def modify_string(self):



        if self.lowercase:
            # erstelle trainingsdatensatz fÃ¼r Fasttext Modell, indem alle Zeilen verdoppelt werden einmal mit .lower()
            ft_training_lower = deepcopy(self.fasttext_df)
            ft_training_lower["taetigk"] = ft_training_lower["taetigk"].apply(lambda x: x.lower())
            ft_data_2 = pd.concat([ft_training_lower, deepcopy(self.fasttext_df)])

        return self.X_ft, self.y_ft

    def lowercase(string:str) -> str:
        return string.lower()

    def remove_stopwords(string:str) -> str:
        stopWords = set(stopwords.words('german'))
        tokenized = word_tokenize(string)
        clean = [w for w in tokenized if w not in stopWords]
        return TreebankWordDetokenizer().detokenize([w for w in tokenized if w not in stopWords])

    def remove_numbers(string:str) -> str:
        ...

    def remove_punctuation(string:str) -> str:
        ...

    def keyboard_augmentation(string:str) -> str:
        aug = nac.KeyboardAug()
        augmented_text = aug.augment(string)
        #print("Original:")
        #print(string)
        #print("Augmented Text:")
        #print(augmented_text)
        return augmented_text

    def synonym_augmentation(string:str) -> str:
        ...